{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwWbxK5N1U3R+m1ZD9q+yE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vignesh0361/Pyspark/blob/main/0_Spark_Sessions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SparkSession** is a unified entry point for Spark applications. Creating a SparkSession instance would be the first statement you would write to program with RDD, DataFrame, and Dataset. SparkSession will be created using SparkSession.builder() builder pattern.\n",
        "\n",
        "SparkSession consolidates several previously separate contexts, such as SQLContext, HiveContext, and StreamingContext, into one entry point, simplifying the interaction with Spark and its different APIs. It enables users to perform various operations like reading data from various sources, executing SQL queries, creating DataFrames and Datasets, and performing actions on distributed datasets efficiently.\n",
        "\n",
        "**For those engaging with Spark through the spark-shell CLI, the ‘spark’ variable automatically provides a default Spark Session, eliminating the need for manual creation within this context. **"
      ],
      "metadata": {
        "id": "MRB9qmHksOu5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kQRe-bjH8PX",
        "outputId": "8e64b598-e64f-4d27-fc4a-7252f71b4555"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7bb56c4bd2d0>\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "print(spark)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_new = SparkSession.newSession\n",
        "print(spark_new)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KbdDQVAy4-L",
        "outputId": "fc0d6500-6356-4559-8cc3-592cabcce001"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<function SparkSession.newSession at 0x7bb58ed971a0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at the two outputs and the spark sessions are different"
      ],
      "metadata": {
        "id": "6vu9F4cQy9mP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Some useful spark commands**"
      ],
      "metadata": {
        "id": "lg54EEEz04AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.version)\n",
        "print(spark.catalog)\n",
        "print(spark.conf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XXhn3vN0XDz",
        "outputId": "6e633b44-ef46-4eb6-fa78-046af8493fc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.4\n",
            "<pyspark.sql.catalog.Catalog object at 0x7bb5a43a1210>\n",
            "<pyspark.sql.conf.RuntimeConfig object at 0x7bb56c2afe10>\n"
          ]
        }
      ]
    }
  ]
}