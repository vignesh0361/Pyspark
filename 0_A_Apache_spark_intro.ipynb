{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQKMJ30WwZvu9eZ2sjOgA4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark is an Open source analytical processing engine for large-scale powerful distributed data processing and machine learning applications"
      ],
      "metadata": {
        "id": "kk297Tz1WsA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark 3.5 is a framework that is supported in Scala, Python, R Programming, and Java. Below are different implementations of Spark.\n",
        "\n",
        "    Spark – Default interface for Scala and Java\n",
        "    PySpark – Python interface for Spark\n",
        "    SparklyR – R interface for Spark.\n"
      ],
      "metadata": {
        "id": "dev1eLPUc5JJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Advantages of Spark**\n",
        "\n",
        "1.   Spark is a general-purpose, **in-memory**, fault-tolerant, **distributed processing engine** that allows you to process data efficiently in a distributed fashion.\n",
        "2.   Applications running on Spark are **100x faster** than traditional systems.\n",
        "3.   You will get great benefits from using Spark for data ingestion pipelines.\n",
        "4.   Using Spark we can process data from Hadoop HDFS, AWS S3, Databricks DBFS, Azure Blob Storage, and many file systems.\n",
        "5.   Spark also is used to process real-time data **using Streaming and Kafka**.\n",
        "6.   Using Spark Streaming you can also stream files from the file system and also stream from the socket.\n",
        "7.   Spark natively has machine learning and graph libraries.\n",
        "8.   Provides connectors to store the data in NoSQL databases like MongoDB.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LnYci67hdGZn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark Context (now called as SparkSession)**\n",
        "\n",
        "SparkContext is available since Spark 1.x (JavaSparkContext for Java) and is used to be an entry point to Spark and PySpark before introducing SparkSession in 2.0. Creating SparkContext was the first step to the program"
      ],
      "metadata": {
        "id": "oPkXT1I2eZ6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Apache Spark Architecture***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Spark works in a master-slave architecture where the master is called the “Driver” and slaves are called “Workers”. When you run a Spark application, Spark Driver creates a context (now called a SparkSession) that is an entry point to your application, and all operations (transformations and actions) are executed on worker nodes, and the resources are managed by Cluster Manager (or Cluster)."
      ],
      "metadata": {
        "id": "mQMgl1w4fCV_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark Job\n",
        "\n",
        "---\n",
        "[link text](https://sparkbyexamples.com/spark/what-is-spark-job/)\n",
        "\n",
        "In Apache Spark, a Spark job is divided into Spark stages, where each stage represents a set of tasks that can be executed in parallel. A stage consists of a set of tasks that are executed on a set of partitions of the data. Data is divided into smaller chunks, called partitions, and processed in parallel across multiple nodes in a cluster. This allows for faster processing and scalability\n",
        "\n",
        "Job --> Stage --> Task\n"
      ],
      "metadata": {
        "id": "CvS_ZoJLg5RS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "val df = spark.read  **//JOB0: read the CSV file**\n",
        "              .option(\"inferSchema\", true) **//JOB1: Inferschema from the file**\n",
        "              .option(\"header\", true)\n",
        "              .csv(\"dbfs:/databricks-datasets/sfo_customer_survey/2013_SFO_Customer_Survey.csv\")\n",
        "\n",
        "df.count **//JOB2: Count the records in the File**\n"
      ],
      "metadata": {
        "id": "23rGDLgXhRTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spark Executor**\n",
        "\n",
        "Spark Executor is a process that runs on a worker node in a Spark cluster and is responsible for executing tasks assigned to it by the Spark driver program."
      ],
      "metadata": {
        "id": "GWP4kvsJhl5z"
      }
    }
  ]
}